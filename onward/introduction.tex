\section{Introduction}

Software testing is of primary importance. In many, if not most, software development projects, testing is the main method of ensuring code quality (above static analysis, runtime monitoring, etc).

Still, there are few methodologies for software testing that are fully automatable. Perhaps the most notable exception to this statement is \emph{delta debugging} \cite{DeltaDebugging:2000,DeltaDebugging:2002}, where the idea is to systematically narrow the state difference between a failing run and a passing run. As an example, if a 
web browser crashes on a 10K-line webpage, then via an iterative process of minimizing the failure-inducing input, delta debugging can determine the particular HTML tag causing the failure.

\subsection{Motivation} This paper follows the same motivation, namely to automate manual reasoning in the process of debugging. Indeed, there is room for more work in this space, since delta debugging --- while broadly applicable and effective --- still leaves important cases uncovered.

We focus on one such case: the notorious problem of detecting, or inducing, failures in the presence of relationships, or dependencies, between different parts of the input. Unlike the example of a webpage that contains a specific bad tag, we are interested in failures that are only triggered if multiple conditions are satisfied simultaneously.

To make this characterization more concrete, we turn to an actual example (which we will dive into in more detail later). In the domain of security testing, the common practice is to pass illegal inputs, containing a security payload, into input points of the target application. A classic example is a web application, whose inputs are GET and POST parameters, cookie and session attributes, etc \cite{TrippIssta:2013}.

If the web application is designed properly, then such parameters --- coming from the user and thus being untrusted --- are checked for maliciousness before they are passed to downstream business logic \cite{Offutt1,Offutt2,Offutt3}. The check may mutate the input, thus being a \emph{sanitizer}, or make an accept/reject decision, thus being a \emph{validator}. In both cases, the implementation typically matches the input against a regular expression to decide its maliciousness.

As a concrete example, given input 
\begin{center}
$\sf{<script>alert('1')</script>}$
\end{center}
which is clearly suspicious, regular expression
\begin{center}
``$\sf{((javascript:)|(</[a-zA-Z0-9]+>)}$"
\end{center}
would match against it, thereby classifying it as malicious, by asserting the simultaneous presence of ${\sf </}$ and ${\sf >}$ (in the opening and closing {\sf script} tags). This relationship between ${\sf </}$ and ${\sf >}$ is nontrivial to capture without direct analysis of the sanitizer. That is, given only the \emph{black-box} ability to send XSS payloads to the web application and register the response, deriving that internally the web application enforces the rule of not letting inputs containing ${\sf </}$ and later ${\sf >}$ through is nonobvious.

Note, importantly, that existing testing methodologies, including delta debugging, are challenged by this problem. Formulating hypotheses iteratively is hard in this case, since the hypothesis cannot simply highlight a particular piece of the input, but rather it has to be (or become) aware of interactions between different parts of the input.

Note, additionally, that there is significant value in developing black-box testing techniques that are able to capture such relationships efficiently. Understanding the circumstances under which an input would be rejected or modified allows the testing tool to craft, or select, another input that bypasses the check. For our example, an alternative input might be
\begin{center}
	${\sf <IFRAME~SRC="javascript\ldots">\quad\sf </IFRAME>}$
	\end{center}
which utilizes the ${\sf javascript:}$ syntax to execute script content (left unspecified above as $\ldots$).

\subsection{Our Approach} We leverage results from the area of combinatorial mathematics and mathematical programming to formulate a novel testing approach. The key conceptual idea is to break up the task of locating elements of interest (in our example, either individual parts of the input, like ${\sf <}$,
${\sf >}$, ${\sf />}$ and ${\sf script}$, or relationships, like ${\sf (<,>)}$ and ${\sf (<,/>)}$), into tests on subsets (or groups) rather than on individual elements. This is known as \emph{group testing}.

The main assumption underlying group testing is that the property of interest is disjunctive in nature. That is, if it holds for any individual in the group, then it holds for the group as a whole. As an illustration, if we craft a test input to the web application that contains ${\sf <}$ and later ${\sf />}$, then regardless of what the other parts of the input are (excluding some end cases like line breaks), it will be rejected.

Thanks to this property, it becomes possible to localize --- through a relatively short series of (properly designed) tests --- the elements present in the input that correlate with rejection. Stated formally, given $N$ candidate elements, let $K \ll N$ be the expected number of elements of interest. Then it is possible to converge on these $K$ elements with high probability by executing $O(K \cdot \log (N))$ tests (or drawing this number of measurements, to use signal-processing terminology).

To allow the reader a concrete notion of how efficient group testing is, if $N=5000$ and $K=5$, then the 5 elements of interest could be located with high probability using on the order of $K\log(N)$ tests where $K\log(N)=50$. A key assumption here is the $K<<N$, i.e., most of the candidate elements do not cause rejection. This allows the definition of relationships between elements as atomic elements while retaining reasonable performance, and hence the possibility to handle complex black-box testing problems for the first time (at least to our knowledge) via a general yet practical solution.

\subsection{Contributions}

This paper makes the following principal contributions:
\begin{enumerate}
	\item \emph{Testing as a Form of Compressed Sensing.} The main contribution of this paper is in demonstrating the relevance and benefit of compressed sensing as a framework for software testing. We view this as the first step and foundation in designing efficient and principled testing techniques, in different domains, based on the theory of compressed sensing.
	\item \emph{Application: Security Testing.} To establish the feasibility of our proposal, we have investigated a particular domain: security testing, and integrity testing in specific. We instantiate the notions of input grouping and randomization in this domain, and explain how signal is recovered based on the output of the security defenses (sanitizers and validators).
	\item \emph{Implementation and Evaluation.} We have created a prototype implementation of the security-testing instance. We report on promising experiments involving the prototype system, which indicate the huge reduction in testing effort (measured in terms of number of tests) thanks to our approach.
\end{enumerate}