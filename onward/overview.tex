\section{Background and Problem Statement}

We begin with some background on security testing, which will serve us later when we instantiate compressed sensing in this context. We then provide a formal statement of the concrete problem that we address in this paper.

\subsection{Security Testing}

Web applications (and services) often make sensitive use of user-provided data. Such data may reach the response HTML, a database record, a session attribute, etc. To mitigate potential threats due to the use of untrusted data in sensitive operations, web applications make use of \emph{validators} and \emph{sanitizers}. Validators are pure functions that make a boolean determination whether or not the input is admissible. Sanitizers, on the other hand, mutate the input, if it is not admissible, such that it becomes admissible. For brevity, we will sometimes refer to sanitizers and validators collectively as \emph{input filters} or \emph{defenses}.

Analogously to other testing algorithms, we assume a predefined set of payloads \cite{TrippIssta:2013,AppScanStd}. While other approaches to testing are based on expert knowledge \cite{AppScanStd} or on an online feedback loop \cite{TrippIssta:2013}, the approach that we are pursuing is model-driven testing: A model is derived based on the black-box system under test, and then --- based on the model --- one or more tests, which are expected to succeed according to the model, are discharged.

In our case, the model is formed by sending random combinations (or groups) of tokens and token tuples that constitute parts of payloads (such as {\sf script}, {\sf /}, {\sf $\backslash$}, {\sf alert}, etc). The model then captures how the input filters operate on inputs, and by implication, if and how they can be bypassed.

\subsection{Problem Statement}

We assume a finite alphabet $\Sigma=\{x_1,\ldots,x_N\}$ of $N$ tokens used to express test inputs. An input is a member of $\Sigma^{*}$ which is the set of strings that can be derived from tokens in $\Sigma$. For example, input ${\sf <script>alert('1')</script>}$ consists of tokens ${\sf <,script,>,alert,(,',),</,script,>}$, along with a single  padding character $\sf{1}$.

A \emph{pattern} is a token tuple, e.g. the 1-ary tuple ${\sf (script)}$ or the 2-ary tuple $\sf{(</,script)}$. Inputs are generated by adding padding between tokens in a given pattern as well as by appending several such padded patterns. We say that input $i=x_1, \ldots ,x_n$ matches pattern $e$ of arity $k$, denoted $i \models e$, if
$$
\exists 1 \leq i_1 \leq \ldots \leq i_k \leq n.\ \bigwedge_{1 \leq m \leq k}
i(i_m) = e(m)  
$$
That is, the input contains a subsequence of tokens that matches the tokens specified as the element.

Input filtering is modeled as a regular-expression membership judgment. This is, in fact, the common way of implementing such filters in practice \cite{Regex}. We follow the standard syntax for expressing a regular expression:
$$
	r = \epsilon\ |\ \{ \sigma \}_{\sigma \in \Sigma}\ |\ r + r\ |\ r \cdot r\ |\ r^{\star}
$$
For simplicity, and without loss of generality, we fix the behavior of the filter, such that given filter $r$ and input $i$, $i$ is in the language of $r$ --- denoted $i \in L_r$ --- iff $r$ \emph{rejects} $i$. That is, the filter matches against the input iff the input is considered illegal.

The problem we target in this paper is to test the correctness of the input filter efficiently under black-box assumptions. Informally, what we mean by ``efficiently'' is that only a small fraction of the test inputs are tried. What we mean by ``black-box assumptions'' is that the testing system has no access to any of the code, and so the system's behavior is based solely on the observed input/output pairs.



